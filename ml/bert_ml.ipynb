{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_full_repo_name' from 'huggingface_hub' (c:\\Users\\haniw\\anaconda3\\envs\\ImageProcessing\\Lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\haniw\\anaconda3\\envs\\ImageProcessing\\Lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\haniw\\anaconda3\\envs\\ImageProcessing\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\haniw\\anaconda3\\envs\\ImageProcessing\\Lib\\site-packages\\transformers\\utils\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_full_repo_name  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HF_HUB_DISABLE_TELEMETRY \u001b[38;5;28;01mas\u001b[39;00m DISABLE_TELEMETRY  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_full_repo_name' from 'huggingface_hub' (c:\\Users\\haniw\\anaconda3\\envs\\ImageProcessing\\Lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#TODO: CHANGE THE FOLDER and SAVE PATHS\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use only GPU 0\n",
    "# Check if the GPU is being used\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU instead.\")\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "# Load Data\n",
    "folder_path_train = \"..\\\\data\\\\cleaned\\\\train\"\n",
    "folder_path_test = \"data\\\\cleaned\\\\test\"\n",
    "names = [\"Facebook\", \"Reddit\", \"Twitter\", \"Youtube\"]\n",
    "dfs_train = {n: pd.read_csv(os.path.join(folder_path_train, f\"{n.lower()}_train.csv\")) for n in names}\n",
    "dfs_test = {n: pd.read_csv(os.path.join(folder_path_test, f\"{n.lower()}_test.csv\")) for n in names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models on one platform and test on others\n",
    "def train_and_evaluate(train_name, test_names, model_name, model_class, tokenizer_class, save_path):\n",
    "    # Prepare training data\n",
    "    df_train = dfs_train[train_name]\n",
    "    df_train['label'] = df_train['label'].astype(int)\n",
    "    train_dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_train = tokenized_train.remove_columns(['text'])\n",
    "    tokenized_train.set_format('torch')\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(tokenized_train, batch_size=4, shuffle=True)  # Reduced batch size\n",
    "\n",
    "    # Load and Train Model\n",
    "    model = model_class.from_pretrained(model_name, num_labels=2)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    def train_model(model, train_loader, optimizer, scheduler, num_epochs=3, accumulation_steps=4):  # Increased accumulation steps\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "\n",
    "            for i, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\")):\n",
    "                inputs = {key: val.to(device) for key, val in batch.items() if key != 'label'}\n",
    "                labels = batch['label'].to(device)\n",
    "                outputs = model(**inputs, labels=labels)\n",
    "                loss = outputs.loss / accumulation_steps\n",
    "                total_train_loss += loss.item()\n",
    "                loss.backward()\n",
    "\n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    torch.cuda.empty_cache()  # Clear GPU cache periodically\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            print(f'Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    train_model(model, train_loader, optimizer, scheduler)\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    # Evaluate on test datasets\n",
    "    results = {}\n",
    "    for test_name in test_names:\n",
    "        df_test = dfs_test[test_name]\n",
    "        df_test['label'] = df_test['label'].astype(int)\n",
    "        test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "        tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "        tokenized_test = tokenized_test.remove_columns(['text'])\n",
    "        tokenized_test.set_format('torch')\n",
    "        test_loader = DataLoader(tokenized_test, batch_size=4, shuffle=False)  # Reduced batch size\n",
    "\n",
    "        def evaluate_model(model, test_loader):\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "                    inputs = {key: val.to(device) for key, val in batch.items() if key != 'label'}\n",
    "                    labels = batch['label'].to(device)\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "                    labels = labels.detach().cpu().numpy()\n",
    "                    predictions.extend(preds)\n",
    "                    true_labels.extend(labels)\n",
    "            \n",
    "            return true_labels, predictions\n",
    "\n",
    "        true_labels, predictions = evaluate_model(model, test_loader)\n",
    "        results[test_name] = classification_report(true_labels, predictions, output_dict=True)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        def plot_confusion_matrix(true_labels, predictions, class_names, platform_name):\n",
    "            cm = confusion_matrix(true_labels, predictions)\n",
    "            cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "            plt.title(f\"Confusion Matrix - {platform_name}\")\n",
    "            plt.ylabel(\"Actual\")\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.show()\n",
    "\n",
    "        plot_confusion_matrix(true_labels, predictions, ['Class 0', 'Class 1'], test_name)\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train on dataset of a platform and test on other platforms using BERT\n",
    "bert_results_facebook = train_and_evaluate(\"Facebook\", [\"Reddit\", \"Twitter\", \"Youtube\"], \"bert-base-uncased\", BertForSequenceClassification, BertTokenizer, save_path='ml/bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results_reddit = train_and_evaluate(\"Reddit\", [\"Facebook\", \"Twitter\", \"Youtube\"], \"bert-base-uncased\", BertForSequenceClassification, BertTokenizer, save_path='ml/bert_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results_twitter = train_and_evaluate(\"Twitter\", [\"Reddit\", \"Facebook\", \"Youtube\"], \"bert-base-uncased\", BertForSequenceClassification, BertTokenizer, save_path='ml/bert_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results_youtube = train_and_evaluate(\"Youtube\", [\"Reddit\", \"Facebook\", \"Twitter\"], \"bert-base-uncased\", BertForSequenceClassification, BertTokenizer, save_path='ml/bert_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot performance metrics across platforms\n",
    "def plot_performance(results, model_name):\n",
    "    platforms = list(results.keys())\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1-score']\n",
    "\n",
    "    for metric in metrics:\n",
    "        values = [results[platform]['weighted avg'][metric] for platform in platforms]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(platforms, values, color='skyblue')\n",
    "        plt.xlabel('Platform')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.title(f'{metric.capitalize()} across platforms for {model_name}')\n",
    "        plt.ylim(0, 1)\n",
    "        for i, v in enumerate(values):\n",
    "            plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center')\n",
    "        plt.show()\n",
    "\n",
    "# Plot performance metrics\n",
    "print(\"BERT Results facebook:\")\n",
    "plot_performance(bert_results_facebook, \"BERT\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance metrics\n",
    "print(\"BERT Results reddit:\")\n",
    "plot_performance(bert_results_reddit, \"BERT\")\n",
    "\n",
    "# Plot performance metrics\n",
    "print(\"BERT Results youtube:\")\n",
    "plot_performance(bert_results_youtube, \"BERT\")\n",
    "\n",
    "# Plot performance metrics\n",
    "print(\"BERT Results twitter:\")\n",
    "plot_performance(bert_results_twitter, \"BERT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
